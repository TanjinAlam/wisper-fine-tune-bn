{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'add-apt-repository' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'apt' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'apt' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!add-apt-repository -y ppa:jonathonf/ffmpeg-4\n",
    "!apt update\n",
    "!apt install -y ffmpeg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'H:\\Wisper-FineTune\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in h:\\wisper-finetune\\venv\\lib\\site-packages (0.10.2.post1)\n",
      "Requirement already satisfied: msgpack>=1.0 in h:\\wisper-finetune\\venv\\lib\\site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in h:\\wisper-finetune\\venv\\lib\\site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in h:\\wisper-finetune\\venv\\lib\\site-packages (from librosa) (0.60.0)\n",
      "Requirement already satisfied: joblib>=0.14 in h:\\wisper-finetune\\venv\\lib\\site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: audioread>=2.1.9 in h:\\wisper-finetune\\venv\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: soxr>=0.3.2 in h:\\wisper-finetune\\venv\\lib\\site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in h:\\wisper-finetune\\venv\\lib\\site-packages (from librosa) (1.5.2)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in h:\\wisper-finetune\\venv\\lib\\site-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in h:\\wisper-finetune\\venv\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: pooch>=1.1 in h:\\wisper-finetune\\venv\\lib\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: scipy>=1.2.0 in h:\\wisper-finetune\\venv\\lib\\site-packages (from librosa) (1.13.1)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in h:\\wisper-finetune\\venv\\lib\\site-packages (from librosa) (2.0.2)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in h:\\wisper-finetune\\venv\\lib\\site-packages (from librosa) (4.12.2)\n",
      "Requirement already satisfied: packaging in h:\\wisper-finetune\\venv\\lib\\site-packages (from lazy-loader>=0.1->librosa) (24.2)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in h:\\wisper-finetune\\venv\\lib\\site-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in h:\\wisper-finetune\\venv\\lib\\site-packages (from pooch>=1.1->librosa) (4.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in h:\\wisper-finetune\\venv\\lib\\site-packages (from pooch>=1.1->librosa) (2.32.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in h:\\wisper-finetune\\venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in h:\\wisper-finetune\\venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.8.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in h:\\wisper-finetune\\venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in h:\\wisper-finetune\\venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in h:\\wisper-finetune\\venv\\lib\\site-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in h:\\wisper-finetune\\venv\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
      "Requirement already satisfied: pycparser in h:\\wisper-finetune\\venv\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'H:\\Wisper-FineTune\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n",
      "WARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'H:\\Wisper-FineTune\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jiwer in h:\\wisper-finetune\\venv\\lib\\site-packages (3.0.5)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in h:\\wisper-finetune\\venv\\lib\\site-packages (from jiwer) (8.1.7)\n",
      "Requirement already satisfied: rapidfuzz<4,>=3 in h:\\wisper-finetune\\venv\\lib\\site-packages (from jiwer) (3.10.1)\n",
      "Requirement already satisfied: colorama in h:\\wisper-finetune\\venv\\lib\\site-packages (from click<9.0.0,>=8.1.3->jiwer) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'H:\\Wisper-FineTune\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in h:\\wisper-finetune\\venv\\lib\\site-packages (4.44.1)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in h:\\wisper-finetune\\venv\\lib\\site-packages (from gradio) (0.32.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in h:\\wisper-finetune\\venv\\lib\\site-packages (from gradio) (4.12.2)\n",
      "Requirement already satisfied: urllib3~=2.0 in h:\\wisper-finetune\\venv\\lib\\site-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: markupsafe~=2.0 in h:\\wisper-finetune\\venv\\lib\\site-packages (from gradio) (2.1.5)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in h:\\wisper-finetune\\venv\\lib\\site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: packaging in h:\\wisper-finetune\\venv\\lib\\site-packages (from gradio) (24.2)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in h:\\wisper-finetune\\venv\\lib\\site-packages (from gradio) (0.0.17)\n",
      "Requirement already satisfied: ruff>=0.2.2 in h:\\wisper-finetune\\venv\\lib\\site-packages (from gradio) (0.7.4)\n",
      "Requirement already satisfied: orjson~=3.0 in h:\\wisper-finetune\\venv\\lib\\site-packages (from gradio) (3.10.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in h:\\wisper-finetune\\venv\\lib\\site-packages (from gradio) (0.26.2)\n",
      "Requirement already satisfied: gradio-client==1.3.0 in h:\\wisper-finetune\\venv\\lib\\site-packages (from gradio) (1.3.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in h:\\wisper-finetune\\venv\\lib\\site-packages (from gradio) (2.0.2)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in h:\\wisper-finetune\\venv\\lib\\site-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in h:\\wisper-finetune\\venv\\lib\\site-packages (from gradio) (4.6.2.post1)\n",
      "Requirement already satisfied: httpx>=0.24.1 in h:\\wisper-finetune\\venv\\lib\\site-packages (from gradio) (0.27.2)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in h:\\wisper-finetune\\venv\\lib\\site-packages (from gradio) (6.4.5)\n",
      "Requirement already satisfied: fastapi<1.0 in h:\\wisper-finetune\\venv\\lib\\site-packages (from gradio) (0.115.5)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in h:\\wisper-finetune\\venv\\lib\\site-packages (from gradio) (6.0.2)\n",
      "Requirement already satisfied: ffmpy in h:\\wisper-finetune\\venv\\lib\\site-packages (from gradio) (0.4.0)\n",
      "Requirement already satisfied: pydub in h:\\wisper-finetune\\venv\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in h:\\wisper-finetune\\venv\\lib\\site-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: semantic-version~=2.0 in h:\\wisper-finetune\\venv\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in h:\\wisper-finetune\\venv\\lib\\site-packages (from gradio) (0.13.0)\n",
      "Requirement already satisfied: matplotlib~=3.0 in h:\\wisper-finetune\\venv\\lib\\site-packages (from gradio) (3.9.2)\n",
      "Requirement already satisfied: jinja2<4.0 in h:\\wisper-finetune\\venv\\lib\\site-packages (from gradio) (3.1.4)\n",
      "Requirement already satisfied: pydantic>=2.0 in h:\\wisper-finetune\\venv\\lib\\site-packages (from gradio) (2.9.2)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in h:\\wisper-finetune\\venv\\lib\\site-packages (from gradio) (10.4.0)\n",
      "Requirement already satisfied: fsspec in h:\\wisper-finetune\\venv\\lib\\site-packages (from gradio-client==1.3.0->gradio) (2024.9.0)\n",
      "Requirement already satisfied: websockets<13.0,>=10.0 in h:\\wisper-finetune\\venv\\lib\\site-packages (from gradio-client==1.3.0->gradio) (12.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in h:\\wisper-finetune\\venv\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in h:\\wisper-finetune\\venv\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in h:\\wisper-finetune\\venv\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in h:\\wisper-finetune\\venv\\lib\\site-packages (from fastapi<1.0->gradio) (0.41.2)\n",
      "Requirement already satisfied: certifi in h:\\wisper-finetune\\venv\\lib\\site-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in h:\\wisper-finetune\\venv\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in h:\\wisper-finetune\\venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in h:\\wisper-finetune\\venv\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (4.67.0)\n",
      "Requirement already satisfied: requests in h:\\wisper-finetune\\venv\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\n",
      "Requirement already satisfied: filelock in h:\\wisper-finetune\\venv\\lib\\site-packages (from huggingface-hub>=0.19.3->gradio) (3.16.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in h:\\wisper-finetune\\venv\\lib\\site-packages (from importlib-resources<7.0,>=1.3->gradio) (3.21.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in h:\\wisper-finetune\\venv\\lib\\site-packages (from matplotlib~=3.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: cycler>=0.10 in h:\\wisper-finetune\\venv\\lib\\site-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in h:\\wisper-finetune\\venv\\lib\\site-packages (from matplotlib~=3.0->gradio) (4.55.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in h:\\wisper-finetune\\venv\\lib\\site-packages (from matplotlib~=3.0->gradio) (1.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in h:\\wisper-finetune\\venv\\lib\\site-packages (from matplotlib~=3.0->gradio) (3.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in h:\\wisper-finetune\\venv\\lib\\site-packages (from matplotlib~=3.0->gradio) (1.4.7)\n",
      "Requirement already satisfied: tzdata>=2022.7 in h:\\wisper-finetune\\venv\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in h:\\wisper-finetune\\venv\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in h:\\wisper-finetune\\venv\\lib\\site-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in h:\\wisper-finetune\\venv\\lib\\site-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
      "Requirement already satisfied: six>=1.5 in h:\\wisper-finetune\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: colorama in h:\\wisper-finetune\\venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.19.3->gradio) (0.4.6)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in h:\\wisper-finetune\\venv\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in h:\\wisper-finetune\\venv\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
      "Requirement already satisfied: click>=8.0.0 in h:\\wisper-finetune\\venv\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in h:\\wisper-finetune\\venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in h:\\wisper-finetune\\venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in h:\\wisper-finetune\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in h:\\wisper-finetune\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the 'H:\\Wisper-FineTune\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets>=2.6.1\n",
    "# !pip install git+https://github.com/huggingface/transformers\n",
    "!pip install librosa\n",
    "!pip install evaluate>=0.30\n",
    "!pip install jiwer\n",
    "!pip install gradio\n",
    "# !pip install -q bitsandbytes datasets accelerate loralib\n",
    "# !pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\Wisper-FineTune\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()\n",
    "from huggingface_hub import HfApi , login\n",
    "\n",
    "# Replace 'your_token_here' with your actual Hugging Face token\n",
    "api = HfApi(token=\"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\")\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 25 00:04:39 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.94                 Driver Version: 560.94         CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3070      WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "|  0%   40C    P8             13W /  220W |     675MiB /   8192MiB |      4%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1544    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A      2996    C+G   ...Programs\\Microsoft VS Code\\Code.exe      N/A      |\n",
      "|    0   N/A  N/A      3680    C+G   ...on\\130.0.2849.80\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A      8436    C+G   ...\\cef\\cef.win7x64\\steamwebhelper.exe      N/A      |\n",
      "|    0   N/A  N/A      8708    C+G   ...les\\Microsoft OneDrive\\OneDrive.exe      N/A      |\n",
      "|    0   N/A  N/A      9384    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     14184    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A     14192    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     14876    C+G   ...x64__fqhk48m1tsma0\\app\\StayFree.exe      N/A      |\n",
      "|    0   N/A  N/A     15244    C+G   ...al\\Discord\\app-1.0.9171\\Discord.exe      N/A      |\n",
      "|    0   N/A  N/A     16980    C+G   ...on\\130.0.2849.80\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     17944    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "|    0   N/A  N/A     19296    C+G   ...pdnekdrzrea0\\XboxGameBarSpotify.exe      N/A      |\n",
      "|    0   N/A  N/A     20796    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A     21044    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     21300    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     25220    C+G   ...crosoft\\Edge\\Application\\msedge.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select CUDA device index\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "# model_name_or_path = \"openai/whisper-large-v2\"\n",
    "# model_name_or_path= \"openai/whisper-medium\"\n",
    "model_name_or_path= \"openai/whisper-base\"\n",
    "language = \"Bengali\"\n",
    "language_abbr = \"bn\"\n",
    "task = \"transcribe\"\n",
    "dataset_name = \"mozilla-foundation/common_voice_17_0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:01<00:00, 15.15files/s]\n",
      "Reading metadata...: 21228it [00:00, 183504.74it/s]es/s]\n",
      "Generating train split: 21228 examples [00:05, 3733.49 examples/s]\n",
      "Reading metadata...: 9327it [00:00, 213320.86it/s]examples/s]\n",
      "Generating validation split: 9327 examples [00:02, 3925.79 examples/s]\n",
      "Reading metadata...: 9327it [00:00, 203286.62it/s]es/s]\n",
      "Generating test split: 9327 examples [00:02, 3860.09 examples/s]\n",
      "Reading metadata...: 997561it [00:05, 195803.77it/s]s/s]\n",
      "Generating other split: 997561 examples [03:57, 4201.76 examples/s]\n",
      "Reading metadata...: 7811it [00:00, 175630.21it/s] examples/s]\n",
      "Generating invalidated split: 7811 examples [00:02, 3878.33 examples/s]\n",
      "Reading metadata...: 44121it [00:00, 192257.91it/s]amples/s]\n",
      "Generating validated split: 44121 examples [00:11, 3892.71 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
      "        num_rows: 30555\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
      "        num_rows: 9327\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "\n",
    "common_voice[\"train\"] = load_dataset(dataset_name, language_abbr, split=\"train+validation\", trust_remote_code=True)\n",
    "common_voice[\"test\"] = load_dataset(dataset_name, language_abbr, split=\"test\", trust_remote_code=True)\n",
    "\n",
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperTokenizer\n",
    "\n",
    "TOKENIZER = WhisperTokenizer.from_pretrained(model_name_or_path, language=language, task=task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:                 ‡¶∏‡¶æ‡¶Å‡¶á‡¶•‡¶ø‡¶Ø‡¶º‡¶æ ‡¶™‡¶∂‡ßç‡¶ö‡¶ø‡¶Æ‡¶¨‡¶ô‡ßç‡¶ó‡ßá‡¶∞ ‡¶∏‡¶Ç‡¶∏‡ßç‡¶ï‡ßÉ‡¶§‡¶ø ‡¶¶‡ßç‡¶¨‡¶æ‡¶∞‡¶æ ‡¶Ö‡¶®‡ßá‡¶ï ‡¶™‡ßç‡¶∞‡¶≠‡¶æ‡¶¨‡¶ø‡¶§‡•§\n",
      "Decoded w/ special:    <|startoftranscript|><|bn|><|transcribe|><|notimestamps|>‡¶∏‡¶æ‡¶Å‡¶á‡¶•‡¶ø‡¶Ø‡¶º‡¶æ ‡¶™‡¶∂‡ßç‡¶ö‡¶ø‡¶Æ‡¶¨‡¶ô‡ßç‡¶ó‡ßá‡¶∞ ‡¶∏‡¶Ç‡¶∏‡ßç‡¶ï‡ßÉ‡¶§‡¶ø ‡¶¶‡ßç‡¶¨‡¶æ‡¶∞‡¶æ ‡¶Ö‡¶®‡ßá‡¶ï ‡¶™‡ßç‡¶∞‡¶≠‡¶æ‡¶¨‡¶ø‡¶§‡•§<|endoftext|>\n",
      "Decoded w/out special: ‡¶∏‡¶æ‡¶Å‡¶á‡¶•‡¶ø‡¶Ø‡¶º‡¶æ ‡¶™‡¶∂‡ßç‡¶ö‡¶ø‡¶Æ‡¶¨‡¶ô‡ßç‡¶ó‡ßá‡¶∞ ‡¶∏‡¶Ç‡¶∏‡ßç‡¶ï‡ßÉ‡¶§‡¶ø ‡¶¶‡ßç‡¶¨‡¶æ‡¶∞‡¶æ ‡¶Ö‡¶®‡ßá‡¶ï ‡¶™‡ßç‡¶∞‡¶≠‡¶æ‡¶¨‡¶ø‡¶§‡•§\n",
      "Are equal:             True\n"
     ]
    }
   ],
   "source": [
    "input_str = common_voice[\"train\"][0][\"sentence\"]\n",
    "labels = TOKENIZER(input_str).input_ids\n",
    "decoded_with_special = TOKENIZER.decode(labels, skip_special_tokens=False)\n",
    "decoded_str = TOKENIZER.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "print(f\"Input:                 {input_str}\")\n",
    "print(f\"Decoded w/ special:    {decoded_with_special}\")\n",
    "print(f\"Decoded w/out special: {decoded_str}\")\n",
    "print(f\"Are equal:             {input_str == decoded_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'sentence'],\n",
      "        num_rows: 30555\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['audio', 'sentence'],\n",
      "        num_rows: 9327\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# common_voice = common_voice.remove_columns(\n",
    "#     [\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\",\"variant\"]\n",
    "# )\n",
    "\n",
    "print(common_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Shirk the data into small \n",
    "# common_voice[\"train\"] = common_voice[\"train\"].select(range(1000))\n",
    "# common_voice[\"test\"] = common_voice[\"test\"].select(range(100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': 'C:\\\\Users\\\\Mahinur\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\857cf2a00b940c8a3fc12f2794b5749f5f9ffcc80a60de75ce726105e8fcdbaa\\\\bn_train_0/common_voice_bn_31564335.mp3', 'array': array([2.84217094e-14, 8.24229573e-13, 1.56319402e-12, ...,\n",
      "       1.58417570e-05, 2.11068655e-05, 1.28124266e-05]), 'sampling_rate': 48000}, 'sentence': '‡¶∏‡¶æ‡¶Å‡¶á‡¶•‡¶ø‡¶Ø‡¶º‡¶æ ‡¶™‡¶∂‡ßç‡¶ö‡¶ø‡¶Æ‡¶¨‡¶ô‡ßç‡¶ó‡ßá‡¶∞ ‡¶∏‡¶Ç‡¶∏‡ßç‡¶ï‡ßÉ‡¶§‡¶ø ‡¶¶‡ßç‡¶¨‡¶æ‡¶∞‡¶æ ‡¶Ö‡¶®‡ßá‡¶ï ‡¶™‡ßç‡¶∞‡¶≠‡¶æ‡¶¨‡¶ø‡¶§‡•§'}\n"
     ]
    }
   ],
   "source": [
    "#see the first 5 examples\n",
    "print(common_voice[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': {'path': 'C:\\\\Users\\\\Mahinur\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\102c1d8b61537072c40282b9920b4c59254d75c8e25aa148baf52d261a495ab9\\\\bn_test_0/common_voice_bn_31622386.mp3', 'array': array([-3.12638804e-13, -6.53699317e-13, -1.02318154e-12, ...,\n",
      "        1.69123613e-08,  2.18496865e-08,  1.34226195e-08]), 'sampling_rate': 48000}, 'sentence': '‡¶ó‡¶≠‡ßÄ‡¶∞ ‡¶ú‡¶≤‡ßá‡¶∞ ‡¶¨‡¶æ‡¶∞‡ßç‡¶• ‡¶ì ‡¶¨‡¶π‡ßÅ‡¶Æ‡ßÅ‡¶ñ‡ßÄ ‡¶ü‡¶æ‡¶∞‡ßç‡¶Æ‡¶ø‡¶®‡¶æ‡¶≤ ‡¶∏‡¶π, ‡¶¨‡¶®‡ßç‡¶¶‡¶∞‡¶ü‡¶ø ‡¶¶‡¶ï‡ßç‡¶∑‡¶§‡¶æ‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶¨‡¶ø‡¶∂‡ßç‡¶¨‡ßá‡¶∞ ‡¶¨‡ßÉ‡¶π‡¶§‡ßç‡¶§‡¶Æ ‡¶¨‡¶æ‡¶≤‡ßç‡¶ï ‡¶ï‡ßç‡¶Ø‡¶æ‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ‡¶∞ ‡¶™‡¶∞‡¶ø‡¶ö‡¶æ‡¶≤‡¶®‡¶æ ‡¶ï‡¶∞‡¶§‡ßá ‡¶∏‡¶ï‡ßç‡¶∑‡¶Æ‡•§'}\n"
     ]
    }
   ],
   "source": [
    "#see the first 5 examples\n",
    "print(common_voice[\"test\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "from transformers import WhisperForConditionalGeneration, WhisperFeatureExtractor, WhisperTokenizer, BitsAndBytesConfig, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import WhisperProcessor\n",
    "PROCESSOR = WhisperProcessor.from_pretrained(model_name_or_path, language=language, task=task)  \n",
    "FEATURE_EXTRACTOR = WhisperFeatureExtractor.from_pretrained(model_name_or_path)\n",
    "MODEL = WhisperForConditionalGeneration.from_pretrained(\n",
    "\tmodel_name_or_path, use_cache=False, device_map=\"auto\",  # in case weird bug in `peft`: device_map={\"\": 0}\n",
    "\tquantization_config=BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\")\n",
    ")\n",
    "MODEL.config.forced_decoder_ids = None\n",
    "MODEL.config.suppress_tokens = []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 884,736 || all params: 73,478,656 || trainable%: 1.2041\n"
     ]
    }
   ],
   "source": [
    "import peft\n",
    "\n",
    "# naive model parallelism setup to train on multi-GPU with PEFT, see: https://github.com/huggingface/peft/issues/242#issuecomment-1491447956\n",
    "if torch.cuda.device_count() > 1:\n",
    "\timport accelerate\n",
    "\tDEV_MAP = MODEL.hf_device_map.copy()\n",
    "\tDEV_MAP[\"model.decoder.embed_tokens\"] = DEV_MAP[\"model.decoder.embed_positions\"] = DEV_MAP[\"proj_out\"] = MODEL._hf_hook.execution_device\n",
    "\taccelerate.dispatch_model(MODEL, device_map=DEV_MAP)\n",
    "\tsetattr(MODEL, \"model_parallel\", True)\n",
    "\tsetattr(MODEL, \"is_parallelizable\", True)\n",
    "# see my other notebook to use distributed data parallelism for more effective gpu usage\n",
    "\n",
    "DUMMY_TOKEN = -100\n",
    "\n",
    "MODEL_BIS = peft.get_peft_model(\n",
    "\tpeft.prepare_model_for_kbit_training(MODEL, use_gradient_checkpointing=True, gradient_checkpointing_kwargs={\"use_reentrant\": False}),\n",
    "\t# peft.LoraConfig(r=32, lora_alpha=64, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=.05, bias=\"none\")\n",
    "\tpeft.LoraConfig(r=24, lora_alpha=40, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=.05, bias=\"none\")\n",
    " \n",
    "\t# peft.AdaLoraConfig(‚Ä¶)  # higher number of trainable parameters\n",
    ")\n",
    "MODEL_BIS.model.model.encoder.conv1.register_forward_hook(lambda module, input, output: output.requires_grad_(True))  # re-enable grad computation for conv layer\n",
    "\n",
    "MODEL_BIS.print_trainable_parameters()  # 16 millions = 1% of 1.6 billions params of whisper large\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] = FEATURE_EXTRACTOR(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = TOKENIZER(batch[\"sentence\"]).input_ids\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30555/30555 [07:40<00:00, 66.41 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9327/9327 [02:21<00:00, 65.83 examples/s]\n"
     ]
    }
   ],
   "source": [
    "FEATURE_EXTRACTOR\n",
    "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sentence'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m input_str \u001b[38;5;241m=\u001b[39m \u001b[43mcommon_voice\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentence\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      2\u001b[0m labels \u001b[38;5;241m=\u001b[39m TOKENIZER(input_str)\u001b[38;5;241m.\u001b[39minput_ids\n\u001b[0;32m      3\u001b[0m decoded_with_special \u001b[38;5;241m=\u001b[39m TOKENIZER\u001b[38;5;241m.\u001b[39mdecode(labels, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'sentence'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "\tdef __call__(self, features):\n",
    "\t\t# split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "\t\tinput_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "\t\tlabel_features = [{\"input_ids\"     : feature[\"labels\"]        } for feature in features]  # get the tokenized label sequences\n",
    "\n",
    "\t\tbatch = FEATURE_EXTRACTOR.pad(input_features, return_tensors=\"pt\")  # treat the audio inputs by simply returning torch tensors\n",
    "\t\tlabels_batch =  TOKENIZER.pad(label_features, return_tensors=\"pt\")  # pad the labels to max length\n",
    "\t\tlabels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), DUMMY_TOKEN)  # replace padding with -100 to ignore loss correctly\n",
    "\n",
    "\t\tif (labels[:, 0] == TOKENIZER.bos_token_id).all().cpu().item():  # if bos token is appended in previous tokenization step,\n",
    "\t\t\tlabels = labels[:, 1:]  # cut bos token here as it‚Äôs append later anyways\n",
    "\n",
    "\t\tbatch[\"labels\"] = labels\n",
    "\t\treturn batch\n",
    "DATA_COLLATOR = DataCollatorSpeechSeq2SeqWithPadding()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = TOKENIZER.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = TOKENIZER.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = TOKENIZER.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall -y transformers\n",
    "# !pip install transformers==4.45.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import WhisperForConditionalGeneration, BitsAndBytesConfig\n",
    "\n",
    "# # Create a BitsAndBytesConfig object for 4-bit quantization\n",
    "# quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "# # Load the model with the quantization configuration\n",
    "# %pip install bitsandbytes\n",
    "# model = WhisperForConditionalGeneration.from_pretrained(\n",
    "#     model_name_or_path,\n",
    "#     quantization_config=quantization_config,\n",
    "#     device_map=\"auto\"  # Use CPU instead of CUDA\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.config.forced_decoder_ids = None\n",
    "# model.config.suppress_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model\n",
    "\n",
    "# config = LoraConfig(r=8, lora_alpha=40, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\")\n",
    "\n",
    "# config = get_peft_model(model, config)\n",
    "# config.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\Wisper-FineTune\\venv\\lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "SAVE_PATH = \"./wisper-bn-lora-with-medium\"  # mount gdrive using GUI before training\n",
    "BATCH_SIZE = 8  # should be a power of 2\n",
    "\n",
    "# colab free tier can only run for 8-12h max daily\n",
    "# kaggle free tier can only run for 30h max weekly but max 12h per session\n",
    "\n",
    "has_bf16 = torch.cuda.is_bf16_supported()  # GPU Ampere or later\n",
    "\n",
    "TRAINING_ARGS = Seq2SeqTrainingArguments(\n",
    "\toutput_dir=SAVE_PATH,\n",
    "\tper_device_train_batch_size=BATCH_SIZE,\n",
    "\t# per_device_eval_batch_size=BATCH_SIZE,\n",
    "\tfp16=not has_bf16,\n",
    "\tbf16=has_bf16, tf32=has_bf16,\n",
    "\t# torch_compile=True,  # SDPA not support whisper yet\n",
    "\t# report_to=[\"tensorboard\"],\n",
    "\tmax_steps=3600,  # no `num_train_epochs` coz streaming\n",
    "\tlogging_steps=25,\n",
    "\tsave_steps=50,\n",
    "\t# eval_steps=50,\n",
    "\tevaluation_strategy=\"no\",  # \"steps\"\n",
    "\tsave_total_limit=3,\n",
    "\n",
    "\toptim=\"adamw_bnb_8bit\",  # 8-bit AdamW optimizer: lower vram usage than default AdamW\n",
    "\tlearning_rate=5e-6,\n",
    "\twarmup_ratio=.05,  # keep between 5-15%\n",
    "\tgradient_accumulation_steps=1 if BATCH_SIZE >= 8 else 8 // BATCH_SIZE,\n",
    "\tremove_unused_columns=False, label_names=[\"labels\"],  # required by PEFT\n",
    "\t# predict_with_generate=True,  # must disable coz PEFT\n",
    ")\n",
    "\n",
    "TRAINER = Seq2SeqTrainer(\n",
    "\targs=TRAINING_ARGS,\n",
    "\tmodel=MODEL_BIS,\n",
    "\ttrain_dataset=common_voice['train'],\n",
    "\tdata_collator=DATA_COLLATOR,\n",
    "    compute_metrics=compute_metrics,\n",
    "\t# compute_metrics=compute_metrics,  # must disable coz PEFT\n",
    "\ttokenizer=FEATURE_EXTRACTOR,  # not TOKENIZER\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 25/3600 [00:28<53:39,  1.11it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5283, 'grad_norm': 6.670492172241211, 'learning_rate': 6.944444444444446e-07, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|‚ñè         | 50/3600 [00:50<50:39,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5302, 'grad_norm': 6.033204555511475, 'learning_rate': 1.3888888888888892e-06, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 75/3600 [01:13<52:18,  1.12it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5017, 'grad_norm': 5.878053665161133, 'learning_rate': 2.0833333333333334e-06, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 100/3600 [01:35<51:29,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4763, 'grad_norm': 5.130723476409912, 'learning_rate': 2.7777777777777783e-06, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñé         | 125/3600 [01:58<51:40,  1.12it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4261, 'grad_norm': 3.6140270233154297, 'learning_rate': 3.4722222222222224e-06, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 150/3600 [02:20<51:17,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3617, 'grad_norm': 2.891312837600708, 'learning_rate': 4.166666666666667e-06, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñç         | 175/3600 [02:42<48:54,  1.17it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2925, 'grad_norm': 2.5319466590881348, 'learning_rate': 4.861111111111111e-06, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|‚ñå         | 200/3600 [03:05<50:34,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2299, 'grad_norm': 2.0024287700653076, 'learning_rate': 4.970760233918129e-06, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|‚ñã         | 225/3600 [03:28<50:40,  1.11it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1692, 'grad_norm': 2.022641181945801, 'learning_rate': 4.9342105263157895e-06, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|‚ñã         | 250/3600 [03:51<50:37,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1061, 'grad_norm': 1.8329463005065918, 'learning_rate': 4.8976608187134504e-06, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|‚ñä         | 275/3600 [04:15<49:03,  1.13it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0694, 'grad_norm': 1.468958854675293, 'learning_rate': 4.861111111111111e-06, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|‚ñä         | 300/3600 [04:37<49:09,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.018, 'grad_norm': 1.8198479413986206, 'learning_rate': 4.824561403508772e-06, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|‚ñâ         | 325/3600 [05:00<50:10,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0126, 'grad_norm': 1.6391319036483765, 'learning_rate': 4.788011695906433e-06, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñâ         | 350/3600 [05:22<46:56,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9555, 'grad_norm': 2.1758439540863037, 'learning_rate': 4.751461988304094e-06, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà         | 375/3600 [05:46<48:52,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9048, 'grad_norm': 1.5108251571655273, 'learning_rate': 4.714912280701755e-06, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|‚ñà         | 400/3600 [06:08<48:26,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8914, 'grad_norm': 2.078788995742798, 'learning_rate': 4.678362573099415e-06, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|‚ñà‚ñè        | 425/3600 [06:31<45:07,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8487, 'grad_norm': 1.7744122743606567, 'learning_rate': 4.641812865497076e-06, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|‚ñà‚ñé        | 450/3600 [06:54<49:24,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.806, 'grad_norm': 1.4066214561462402, 'learning_rate': 4.605263157894737e-06, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|‚ñà‚ñé        | 475/3600 [07:18<47:59,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8041, 'grad_norm': 1.6002869606018066, 'learning_rate': 4.568713450292398e-06, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|‚ñà‚ñç        | 500/3600 [07:41<48:07,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7639, 'grad_norm': 1.791487455368042, 'learning_rate': 4.532163742690059e-06, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|‚ñà‚ñç        | 525/3600 [08:05<46:20,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7489, 'grad_norm': 1.6011313199996948, 'learning_rate': 4.4956140350877196e-06, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|‚ñà‚ñå        | 550/3600 [08:27<46:29,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7352, 'grad_norm': 1.5033060312271118, 'learning_rate': 4.4590643274853805e-06, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|‚ñà‚ñå        | 575/3600 [08:51<46:32,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7172, 'grad_norm': 1.8557298183441162, 'learning_rate': 4.422514619883041e-06, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|‚ñà‚ñã        | 600/3600 [09:14<47:03,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6946, 'grad_norm': 2.1166512966156006, 'learning_rate': 4.385964912280702e-06, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|‚ñà‚ñã        | 625/3600 [09:38<45:19,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6879, 'grad_norm': 1.479943037033081, 'learning_rate': 4.349415204678362e-06, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|‚ñà‚ñä        | 650/3600 [10:00<42:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6882, 'grad_norm': 1.7871661186218262, 'learning_rate': 4.312865497076023e-06, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|‚ñà‚ñâ        | 675/3600 [10:23<44:12,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6633, 'grad_norm': 1.9821395874023438, 'learning_rate': 4.276315789473684e-06, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|‚ñà‚ñâ        | 700/3600 [10:46<44:16,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6704, 'grad_norm': 1.9997961521148682, 'learning_rate': 4.239766081871345e-06, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|‚ñà‚ñà        | 725/3600 [11:09<43:46,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6403, 'grad_norm': 1.887158751487732, 'learning_rate': 4.203216374269006e-06, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|‚ñà‚ñà        | 750/3600 [11:32<43:32,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6332, 'grad_norm': 1.8885308504104614, 'learning_rate': 4.166666666666667e-06, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|‚ñà‚ñà‚ñè       | 775/3600 [11:55<43:07,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6221, 'grad_norm': 1.7942323684692383, 'learning_rate': 4.130116959064328e-06, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|‚ñà‚ñà‚ñè       | 800/3600 [12:18<42:04,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6209, 'grad_norm': 2.1834259033203125, 'learning_rate': 4.093567251461989e-06, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|‚ñà‚ñà‚ñé       | 825/3600 [12:41<43:16,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6027, 'grad_norm': 1.876097559928894, 'learning_rate': 4.05701754385965e-06, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|‚ñà‚ñà‚ñé       | 850/3600 [13:05<41:54,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5891, 'grad_norm': 2.3719210624694824, 'learning_rate': 4.0204678362573105e-06, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|‚ñà‚ñà‚ñç       | 875/3600 [13:28<41:16,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5922, 'grad_norm': 2.0240323543548584, 'learning_rate': 3.983918128654971e-06, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|‚ñà‚ñà‚ñå       | 900/3600 [13:50<39:22,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5827, 'grad_norm': 2.492417573928833, 'learning_rate': 3.947368421052632e-06, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|‚ñà‚ñà‚ñå       | 925/3600 [14:13<38:02,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5728, 'grad_norm': 1.7451690435409546, 'learning_rate': 3.910818713450293e-06, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|‚ñà‚ñà‚ñã       | 950/3600 [14:36<40:26,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5653, 'grad_norm': 1.7540743350982666, 'learning_rate': 3.874269005847954e-06, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|‚ñà‚ñà‚ñã       | 975/3600 [15:00<40:05,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5642, 'grad_norm': 1.8108025789260864, 'learning_rate': 3.837719298245615e-06, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|‚ñà‚ñà‚ñä       | 1000/3600 [15:23<40:03,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5659, 'grad_norm': 2.1595993041992188, 'learning_rate': 3.801169590643275e-06, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|‚ñà‚ñà‚ñä       | 1025/3600 [15:46<39:22,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5651, 'grad_norm': 1.9784917831420898, 'learning_rate': 3.764619883040936e-06, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|‚ñà‚ñà‚ñâ       | 1050/3600 [16:08<38:28,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5393, 'grad_norm': 2.010925769805908, 'learning_rate': 3.728070175438597e-06, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|‚ñà‚ñà‚ñâ       | 1075/3600 [16:32<36:12,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5292, 'grad_norm': 2.1685688495635986, 'learning_rate': 3.691520467836258e-06, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|‚ñà‚ñà‚ñà       | 1100/3600 [16:54<38:11,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5324, 'grad_norm': 1.8930473327636719, 'learning_rate': 3.6549707602339187e-06, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|‚ñà‚ñà‚ñà‚ñè      | 1125/3600 [17:17<37:14,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5236, 'grad_norm': 3.017631769180298, 'learning_rate': 3.618421052631579e-06, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|‚ñà‚ñà‚ñà‚ñè      | 1150/3600 [17:40<37:30,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5197, 'grad_norm': 2.161668539047241, 'learning_rate': 3.58187134502924e-06, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|‚ñà‚ñà‚ñà‚ñé      | 1175/3600 [18:04<36:22,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5164, 'grad_norm': 1.9556732177734375, 'learning_rate': 3.5453216374269006e-06, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|‚ñà‚ñà‚ñà‚ñé      | 1200/3600 [18:26<36:28,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5015, 'grad_norm': 2.07886004447937, 'learning_rate': 3.5087719298245615e-06, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|‚ñà‚ñà‚ñà‚ñç      | 1225/3600 [18:50<34:46,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5126, 'grad_norm': 1.7921712398529053, 'learning_rate': 3.4722222222222224e-06, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|‚ñà‚ñà‚ñà‚ñç      | 1250/3600 [19:12<35:12,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5093, 'grad_norm': 2.062267780303955, 'learning_rate': 3.4356725146198833e-06, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|‚ñà‚ñà‚ñà‚ñå      | 1275/3600 [19:35<34:45,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4915, 'grad_norm': 2.5259478092193604, 'learning_rate': 3.399122807017544e-06, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|‚ñà‚ñà‚ñà‚ñå      | 1300/3600 [19:57<34:26,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4892, 'grad_norm': 1.5859304666519165, 'learning_rate': 3.362573099415205e-06, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|‚ñà‚ñà‚ñà‚ñã      | 1325/3600 [20:20<33:44,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.492, 'grad_norm': 1.8361327648162842, 'learning_rate': 3.326023391812866e-06, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|‚ñà‚ñà‚ñà‚ñä      | 1350/3600 [20:43<34:10,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4935, 'grad_norm': 2.4869158267974854, 'learning_rate': 3.289473684210527e-06, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|‚ñà‚ñà‚ñà‚ñä      | 1375/3600 [21:07<34:01,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4905, 'grad_norm': 2.383180856704712, 'learning_rate': 3.252923976608187e-06, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|‚ñà‚ñà‚ñà‚ñâ      | 1400/3600 [21:29<33:50,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4878, 'grad_norm': 2.12690806388855, 'learning_rate': 3.216374269005848e-06, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|‚ñà‚ñà‚ñà‚ñâ      | 1425/3600 [21:53<33:10,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4877, 'grad_norm': 2.2712388038635254, 'learning_rate': 3.179824561403509e-06, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 1450/3600 [22:15<30:03,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4658, 'grad_norm': 2.2468249797821045, 'learning_rate': 3.1432748538011697e-06, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|‚ñà‚ñà‚ñà‚ñà      | 1475/3600 [22:39<32:09,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4806, 'grad_norm': 2.154968738555908, 'learning_rate': 3.1067251461988306e-06, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1500/3600 [23:01<29:51,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4675, 'grad_norm': 2.164823055267334, 'learning_rate': 3.0701754385964915e-06, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1525/3600 [23:24<31:55,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.455, 'grad_norm': 2.6898553371429443, 'learning_rate': 3.0336257309941524e-06, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1550/3600 [23:47<30:34,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4527, 'grad_norm': 2.5886142253875732, 'learning_rate': 2.9970760233918133e-06, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1575/3600 [24:10<28:54,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4699, 'grad_norm': 1.791033148765564, 'learning_rate': 2.960526315789474e-06, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 1600/3600 [24:32<28:59,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4513, 'grad_norm': 3.1527998447418213, 'learning_rate': 2.9239766081871347e-06, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1625/3600 [24:53<28:10,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4436, 'grad_norm': 2.1751363277435303, 'learning_rate': 2.8874269005847956e-06, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1650/3600 [25:15<27:32,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.441, 'grad_norm': 2.1171631813049316, 'learning_rate': 2.8508771929824565e-06, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1675/3600 [25:36<27:36,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4546, 'grad_norm': 1.5958068370819092, 'learning_rate': 2.8143274853801174e-06, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1700/3600 [25:58<26:56,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4374, 'grad_norm': 2.357860565185547, 'learning_rate': 2.7777777777777783e-06, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1725/3600 [26:20<28:38,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4491, 'grad_norm': 3.10664701461792, 'learning_rate': 2.741228070175439e-06, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1750/3600 [26:43<26:59,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4429, 'grad_norm': 2.2622642517089844, 'learning_rate': 2.7046783625730997e-06, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1775/3600 [27:05<26:30,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4387, 'grad_norm': 2.3476245403289795, 'learning_rate': 2.66812865497076e-06, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1800/3600 [27:27<27:18,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4455, 'grad_norm': 3.1069531440734863, 'learning_rate': 2.631578947368421e-06, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1825/3600 [27:49<26:18,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4375, 'grad_norm': 2.3185694217681885, 'learning_rate': 2.595029239766082e-06, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1850/3600 [28:11<25:09,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4371, 'grad_norm': 2.9889354705810547, 'learning_rate': 2.558479532163743e-06, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1875/3600 [28:34<25:07,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4255, 'grad_norm': 2.2451722621917725, 'learning_rate': 2.521929824561404e-06, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1900/3600 [28:56<25:05,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4147, 'grad_norm': 2.6167802810668945, 'learning_rate': 2.4853801169590643e-06, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1905/3600 [29:01<26:25,  1.07it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mTRAINER\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# resume_from_checkpoint=True  # only if resume\u001b[39;00m\n",
      "File \u001b[1;32mh:\\Wisper-FineTune\\venv\\lib\\site-packages\\transformers\\trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mh:\\Wisper-FineTune\\venv\\lib\\site-packages\\transformers\\trainer.py:2345\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2342\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2344\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 2345\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   2346\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minclude_num_input_tokens_seen:\n",
      "File \u001b[1;32mh:\\Wisper-FineTune\\venv\\lib\\site-packages\\accelerate\\data_loader.py:563\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    561\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_blocking)\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_state_dict()\n\u001b[1;32m--> 563\u001b[0m next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_batches:\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
      "File \u001b[1;32mh:\\Wisper-FineTune\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mh:\\Wisper-FineTune\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mh:\\Wisper-FineTune\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mh:\\Wisper-FineTune\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py:2766\u001b[0m, in \u001b[0;36mDataset.__getitems__\u001b[1;34m(self, keys)\u001b[0m\n\u001b[0;32m   2764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitems__\u001b[39m(\u001b[38;5;28mself\u001b[39m, keys: List) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List:\n\u001b[0;32m   2765\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2766\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2767\u001b[0m     n_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(batch))])\n\u001b[0;32m   2768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [{col: array[i] \u001b[38;5;28;01mfor\u001b[39;00m col, array \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_examples)]\n",
      "File \u001b[1;32mh:\\Wisper-FineTune\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py:2762\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2760\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[0;32m   2761\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mh:\\Wisper-FineTune\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py:2747\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[1;34m(self, key, **kwargs)\u001b[0m\n\u001b[0;32m   2745\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m   2746\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m query_table(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, key, indices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices)\n\u001b[1;32m-> 2747\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[0;32m   2749\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2750\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[1;32mh:\\Wisper-FineTune\\venv\\lib\\site-packages\\datasets\\formatting\\formatting.py:639\u001b[0m, in \u001b[0;36mformat_table\u001b[1;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[0;32m    637\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39mformatter\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    641\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "File \u001b[1;32mh:\\Wisper-FineTune\\venv\\lib\\site-packages\\datasets\\formatting\\formatting.py:407\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[1;34m(self, pa_table, query_type)\u001b[0m\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_column(pa_table)\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mh:\\Wisper-FineTune\\venv\\lib\\site-packages\\datasets\\formatting\\formatting.py:455\u001b[0m, in \u001b[0;36mPythonFormatter.format_batch\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlazy:\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LazyBatch(pa_table, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 455\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_arrow_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    456\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_features_decoder\u001b[38;5;241m.\u001b[39mdecode_batch(batch)\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "TRAINER.train()  # resume_from_checkpoint=True  # only if resume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "TRAINER.save_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # COMPUTE WORD ERROR RATE\n",
    "\n",
    "# from transformers import WhisperProcessor\n",
    "# import evaluate\n",
    "\n",
    "\n",
    "# metric = evaluate.load(\"wer\")\n",
    "\n",
    "# processor = WhisperProcessor.from_pretrained('piashtanjin/wisper-bn-lora',language=language, task = task,tokenizer=TOKENIZER, feature_extractor=FEATURE_EXTRACTOR)\n",
    "\n",
    "# DATA_COLLATOR = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "#     processor = processor,\n",
    "#     decoder_start_token_id = TOKENIZER.bos_token_id\n",
    "# )\n",
    "\n",
    "\n",
    "# def compute_metrics(pred):\n",
    "#     pred_ids = pred.predictions\n",
    "#     label_ids = pred.label_ids\n",
    "\n",
    "#     # replace -100 with the pad_token_id\n",
    "#     label_ids[label_ids == -100] = TOKENIZER.pad_token_id\n",
    "\n",
    "#     # we do not want to group tokens when computing the metrics\n",
    "#     pred_str = TOKENIZER.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "#     label_str = TOKENIZER.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "#     wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "#     return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()\n",
    "from huggingface_hub import HfApi , login\n",
    "\n",
    "# Replace 'your_token_here' with your actual Hugging Face token\n",
    "api = HfApi(token=\"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\")\n",
    "login(token=\"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\")\n",
    "#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\n",
    "#READ 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adapter_model.safetensors:   0%|          | 0.00/1.78M [00:00<?, ?B/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "training_args.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.37k/5.37k [00:00<00:00, 12.9kB/s].14MB/s]\n",
      "events.out.tfevents.1732374254.DESKTOP-JRULPMU.16668.1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37.0k/37.0k [00:00<00:00, 63.5kB/s]\n",
      "adapter_model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.78M/1.78M [00:01<00:00, 907kB/s] \n",
      "\n",
      "Upload 3 LFS files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/piashtanjin/wisper-bn-lora/commit/33bfad279dc80ea62155f3d7b026a031f15aac25', commit_message='piashtanjin/wisper-bn-with-lora', commit_description='', oid='33bfad279dc80ea62155f3d7b026a031f15aac25', pr_url=None, repo_url=RepoUrl('https://huggingface.co/piashtanjin/wisper-bn-lora', endpoint='https://huggingface.co', repo_type='model', repo_id='piashtanjin/wisper-bn-lora'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwargs = {\n",
    "    \"dataset_tags\": \"mozilla-foundation/common_voice_17_0\",\n",
    "    \"dataset\": \"Common Voice 17.0\",  # a 'pretty' name for the training dataset\n",
    "    \"dataset_args\": \"config: bn, split: test\",\n",
    "    \"language\": \"bn\",\n",
    "    \"model_name\": \"Whisper Small  - Tanjin Alam\",  # a 'pretty' name for your model\n",
    "    \"finetuned_from\": \"openai/whisper-tiny\",\n",
    "    \"tasks\": \"speech-to-text\",\n",
    "}\n",
    "peft_model_id = \"piashtanjin/wisper-bn-with-lora\"\n",
    "# Push the model to the Hugging Face Hub\n",
    "TRAINER.push_to_hub(peft_model_id, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import WhisperForConditionalGeneration, Seq2SeqTrainer\n",
    "\n",
    "peft_model_id = \"./wisper-bn-lora\" # Use the same model ID as before.\n",
    "peft_config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    peft_config.base_model_name_or_path, load_in_8bit=True, device_map=\"auto\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s]C:\\Users\\Mahinur\\AppData\\Local\\Temp\\ipykernel_16668\\3254887117.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "h:\\Wisper-FineTune\\venv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [02:24<00:00, 11.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wer=527.9411764705882 and normalized_wer=196.7198964177816\n",
      "{'eval/wer': 527.9411764705882, 'eval/normalized_wer': 196.7198964177816}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "\n",
    "eval_dataloader = DataLoader(common_voice[\"test\"], batch_size=8, collate_fn=DATA_COLLATOR)\n",
    "forced_decoder_ids = PROCESSOR.get_decoder_prompt_ids(language=language, task=task)\n",
    "normalizer = BasicTextNormalizer()\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "normalized_predictions = []\n",
    "normalized_references = []\n",
    "\n",
    "model.eval()\n",
    "for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "    with torch.cuda.amp.autocast():\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = (\n",
    "                model.generate(\n",
    "                    input_features=batch[\"input_features\"].to(\"cuda\"),\n",
    "                    forced_decoder_ids=forced_decoder_ids,\n",
    "                    max_new_tokens=255,\n",
    "                )\n",
    "                .cpu()\n",
    "                .numpy()\n",
    "            )\n",
    "            labels = batch[\"labels\"].cpu().numpy()\n",
    "            labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)\n",
    "            decoded_preds = processor.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "            decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "            predictions.extend(decoded_preds)\n",
    "            references.extend(decoded_labels)\n",
    "            normalized_predictions.extend([normalizer(pred).strip() for pred in decoded_preds])\n",
    "            normalized_references.extend([normalizer(label).strip() for label in decoded_labels])\n",
    "        del generated_tokens, labels, batch\n",
    "    gc.collect()\n",
    "wer = 100 * metric.compute(predictions=predictions, references=references)\n",
    "normalized_wer = 100 * metric.compute(predictions=normalized_predictions, references=normalized_references)\n",
    "eval_metrics = {\"eval/wer\": wer, \"eval/normalized_wer\": normalized_wer}\n",
    "\n",
    "print(f\"{wer=} and {normalized_wer=}\")\n",
    "print(eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://06bcbee1f875225de5.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://06bcbee1f875225de5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mahinur\\AppData\\Local\\Temp\\ipykernel_16668\\3833966753.py:30: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Traceback (most recent call last):\n",
      "  File \"h:\\Wisper-FineTune\\venv\\lib\\site-packages\\gradio\\queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"h:\\Wisper-FineTune\\venv\\lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"h:\\Wisper-FineTune\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 1935, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"h:\\Wisper-FineTune\\venv\\lib\\site-packages\\gradio\\blocks.py\", line 1520, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"h:\\Wisper-FineTune\\venv\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"h:\\Wisper-FineTune\\venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2441, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"h:\\Wisper-FineTune\\venv\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 943, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"h:\\Wisper-FineTune\\venv\\lib\\site-packages\\gradio\\utils.py\", line 826, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\Mahinur\\AppData\\Local\\Temp\\ipykernel_16668\\3833966753.py\", line 31, in transcribe\n",
      "    text = pipe(audio, generate_kwargs={\"forced_decoder_ids\": forced_decoder_ids}, max_new_tokens=255)[\"text\"]\n",
      "  File \"h:\\Wisper-FineTune\\venv\\lib\\site-packages\\transformers\\pipelines\\automatic_speech_recognition.py\", line 284, in __call__\n",
      "    return super().__call__(inputs, **kwargs)\n",
      "  File \"h:\\Wisper-FineTune\\venv\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 1260, in __call__\n",
      "    return next(\n",
      "  File \"h:\\Wisper-FineTune\\venv\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py\", line 124, in __next__\n",
      "    item = next(self.iterator)\n",
      "  File \"h:\\Wisper-FineTune\\venv\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py\", line 269, in __next__\n",
      "    processed = self.infer(next(self.iterator), **self.params)\n",
      "  File \"h:\\Wisper-FineTune\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 701, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"h:\\Wisper-FineTune\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 757, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"h:\\Wisper-FineTune\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 33, in fetch\n",
      "    data.append(next(self.dataset_iter))\n",
      "  File \"h:\\Wisper-FineTune\\venv\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py\", line 186, in __next__\n",
      "    processed = next(self.subiterator)\n",
      "  File \"h:\\Wisper-FineTune\\venv\\lib\\site-packages\\transformers\\pipelines\\automatic_speech_recognition.py\", line 409, in preprocess\n",
      "    raise TypeError(f\"We expect a numpy ndarray as input, got `{type(inputs)}`\")\n",
      "TypeError: We expect a numpy ndarray as input, got `<class 'NoneType'>`\n",
      "h:\\Wisper-FineTune\\venv\\lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:496: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "h:\\Wisper-FineTune\\venv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import gradio as gr\n",
    "from transformers import (\n",
    "    AutomaticSpeechRecognitionPipeline,\n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperTokenizer,\n",
    "    WhisperProcessor,\n",
    ")\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "\n",
    "peft_model_id = \"piashtanjin/wisper-bn-lora\"\n",
    "language = language\n",
    "task = task\n",
    "peft_config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    peft_config.base_model_name_or_path, load_in_8bit=True, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)\n",
    "tokenizer = WhisperTokenizer.from_pretrained(peft_config.base_model_name_or_path, language=language, task=task)\n",
    "processor = WhisperProcessor.from_pretrained(peft_config.base_model_name_or_path, language=language, task=task)\n",
    "feature_extractor = processor.feature_extractor\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=language, task=task)\n",
    "pipe = AutomaticSpeechRecognitionPipeline(model=model, tokenizer=tokenizer, feature_extractor=feature_extractor)\n",
    "\n",
    "\n",
    "def transcribe(audio):\n",
    "    with torch.cuda.amp.autocast():\n",
    "        text = pipe(audio, generate_kwargs={\"forced_decoder_ids\": forced_decoder_ids}, max_new_tokens=255)[\"text\"]\n",
    "    return text\n",
    "mp3_path = \"chunk_150.mp3\"  # Update the path accordingly\n",
    "\n",
    "# Load Whisper model (Turbo version)\n",
    "\n",
    "# Load the MP3 file\n",
    "speech_array, sampling_rate = torchaudio.load(mp3_path, format=\"mp3\")\n",
    "speech_array = speech_array[0].numpy()\n",
    "\n",
    "\n",
    "# Gradio Interface\n",
    "iface = gr.Interface(\n",
    "    fn=transcribe,\n",
    "    inputs=gr.Audio(type=\"filepath\", label=\"Recording\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"PEFT LoRA + INT8 Whisper Bengali\",\n",
    "    description=\"Realtime demo for Bengali speech recognition using `PEFT-LoRA+INT8` fine-tuned Whisper model.\",\n",
    ")\n",
    "\n",
    "iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
